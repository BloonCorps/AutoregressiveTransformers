{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer on Internal Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorboardX\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2namespace(config):\n",
    "    namespace = argparse.Namespace()\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            new_value = dict2namespace(value)\n",
    "        else:\n",
    "            new_value = value\n",
    "        setattr(namespace, key, new_value)\n",
    "    return namespace\n",
    "\n",
    "def parse_args_and_config():\n",
    "    \"\"\"\n",
    "    :return args, config: namespace objects that stores information in args and config files.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=globals()['__doc__'])\n",
    "\n",
    "    parser.add_argument('--config', type=str, default='transformer_tiny.yml', help='Path to the config file')\n",
    "    parser.add_argument('--doc', type=str, default='0', help='A string for documentation purpose')\n",
    "    parser.add_argument('--verbose', type=str, default='info', help='Verbose level: info | debug | warning | critical')\n",
    "    parser.add_argument('--sample', action='store_true', help='Sample at train time')\n",
    "\n",
    "    args, unknown = parser.parse_known_args() #special modification for Jupyter notebooks. \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    args.log = os.path.join('transformer_logs', args.doc)\n",
    "    # parse config file\n",
    "    with open(os.path.join('configs', args.config), 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    new_config = dict2namespace({**config, **vars(args)})\n",
    "\n",
    "    if os.path.exists(args.log):\n",
    "        shutil.rmtree(args.log)\n",
    "\n",
    "    os.makedirs(args.log)\n",
    "\n",
    "    with open(os.path.join(args.log, 'config.yml'), 'w') as f:\n",
    "        yaml.dump(new_config, f, default_flow_style=False)\n",
    "\n",
    "    # setup logger\n",
    "    level = getattr(logging, args.verbose.upper(), None)\n",
    "    if not isinstance(level, int):\n",
    "        raise ValueError('level {} not supported'.format(args.verbose))\n",
    "\n",
    "    handler1 = logging.StreamHandler()\n",
    "    handler2 = logging.FileHandler(os.path.join(args.log, 'stdout.txt'))\n",
    "    formatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')\n",
    "    handler1.setFormatter(formatter)\n",
    "    handler2.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # add device information to args\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    logging.info(\"Using device: {}\".format(device))\n",
    "    new_config.device = device\n",
    "\n",
    "    # set random seed\n",
    "    torch.manual_seed(new_config.seed)\n",
    "    torch.cuda.manual_seed_all(new_config.seed)\n",
    "    np.random.seed(new_config.seed)\n",
    "    logging.info(\"Run name: {}\".format(args.doc))\n",
    "\n",
    "    return args, new_config\n",
    "\n",
    "def get_lr(step, config):\n",
    "    warmup_steps = config.optim.warmup\n",
    "    lr_base = config.optim.lr * 0.002 # for Adam correction\n",
    "    ret = 5000. * config.model.hidden_size ** (-0.5) * \\\n",
    "          np.min([(step + 1) * warmup_steps ** (-1.5), (step + 1) ** (-0.5)])\n",
    "    return ret * lr_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - <ipython-input-17-fea14b691037> - 2022-07-11 15:37:26,139 - Using device: cuda\n",
      "INFO - <ipython-input-17-fea14b691037> - 2022-07-11 15:37:26,139 - Using device: cuda\n",
      "INFO - <ipython-input-17-fea14b691037> - 2022-07-11 15:37:26,140 - Run name: 0\n",
      "INFO - <ipython-input-17-fea14b691037> - 2022-07-11 15:37:26,140 - Run name: 0\n"
     ]
    }
   ],
   "source": [
    "args, config = parse_args_and_config()\n",
    "tb_logger = tensorboardX.SummaryWriter(log_dir=os.path.join('transformer_logs', args.doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CIRCULAR_VALUES = 6280 #We have circular values to the first decimal place. \n",
    "\n",
    "def logsoftmax(x): #supposedly this is numerically stable\n",
    "    m = torch.max(x, -1, keepdim=True).values\n",
    "    return x - m - torch.log(torch.exp(x - m).sum(-1, keepdim=True))\n",
    "\n",
    "def logsumexp(x): #supposedly this is numerically stable\n",
    "    m = x.max(-1).values\n",
    "    return m + torch.log(torch.exp(x - m[...,None]).sum(-1))\n",
    "\n",
    "class ImageTransformer(nn.Module):\n",
    "    \"\"\"ImageTransformer with DMOL or categorical distribution.\"\"\"\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        #hidden_size = 8\n",
    "        #nlayers = 12\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hparams) for layer in range(hparams.nlayers)])\n",
    "\n",
    "        self.input_dropout = nn.Dropout(p=hparams.dropout)\n",
    "\n",
    "        self.embeds = nn.Embedding(NUM_CIRCULAR_VALUES, self.hparams.hidden_size)\n",
    "        \n",
    "        self.output_dense = nn.Linear(self.hparams.hidden_size, NUM_CIRCULAR_VALUES, bias=True)\n",
    "\n",
    "        \n",
    "    def add_timing_signal(self, X, min_timescale=1.0, max_timescale=1.0e4):\n",
    "        num_dims = len(X.shape) - 2 # 2 corresponds to batch and hidden_size dimensions\n",
    "        num_timescales = self.hparams.hidden_size // (num_dims * 2)\n",
    "        log_timescale_increment = np.log(max_timescale / min_timescale) / (num_timescales - 1)\n",
    "        inv_timescales = min_timescale * torch.exp((torch.arange(num_timescales).float() * -log_timescale_increment))\n",
    "        inv_timescales = inv_timescales.to(X.device)\n",
    "        total_signal = torch.zeros_like(X) # Only for debugging purposes\n",
    "        for dim in range(num_dims):\n",
    "            length = X.shape[dim + 1] # add 1 to exclude batch dim\n",
    "            position = torch.arange(length).float().to(X.device)\n",
    "            scaled_time = position.view(-1, 1) * inv_timescales.view(1, -1)\n",
    "            signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)\n",
    "            prepad = dim * 2 * num_timescales\n",
    "            postpad = self.hparams.hidden_size - (dim + 1) * 2 * num_timescales\n",
    "            signal = F.pad(signal, (prepad, postpad))\n",
    "            for _ in range(1 + dim):\n",
    "                signal = signal.unsqueeze(0)\n",
    "            for _ in range(num_dims - 1 - dim):\n",
    "                signal = signal.unsqueeze(-2)\n",
    "            X += signal\n",
    "            total_signal += signal\n",
    "        return X\n",
    "\n",
    "    def shift_and_pad_(self, X):\n",
    "        # Shift inputs over by 1 and pad\n",
    "        shape = X.shape\n",
    "        X = X.view(shape[0], shape[1] * shape[2], shape[3])\n",
    "        X = X[:,:-1,:]\n",
    "        X = F.pad(X, (0, 0, 1, 0)) # Pad second to last dimension\n",
    "        X = X.view(shape)\n",
    "        return X\n",
    "\n",
    "    def forward(self, X, sampling=False):\n",
    "        print(X)\n",
    "        # Reshape inputs\n",
    "        if sampling:\n",
    "            curr_infer_length = X.shape[1]\n",
    "            row_size = self.hparams.image_size * self.hparams.channels\n",
    "            nrows = curr_infer_length // row_size + 1\n",
    "            X = F.pad(X, (0, nrows * row_size - curr_infer_length))\n",
    "            X = X.view(X.shape[0], -1, row_size)\n",
    "        else:\n",
    "            X = X.permute([0, 2, 3, 1]).contiguous()\n",
    "            X = X.view(X.shape[0], X.shape[1], X.shape[2] * X.shape[3]) # Flatten channels into width\n",
    "\n",
    "        #elif self.hparams.distr == \"cat\":\n",
    "        # Convert to indexes, and use separate embeddings for different channels\n",
    "        X = (X * (NUM_PIXELS - 1)).long()\n",
    "        channel_addition = (torch.tensor([0, 1, 2]) * NUM_PIXELS).to(X.device).repeat(X.shape[2] // 3).view(1, 1, -1)\n",
    "        X += channel_addition\n",
    "        X = self.embeds(X) * (self.hparams.hidden_size ** 0.5)\n",
    "\n",
    "        X = self.shift_and_pad_(X)\n",
    "        X = self.add_timing_signal(X)\n",
    "        shape = X.shape\n",
    "        X = X.view(shape[0], -1, shape[3])\n",
    "\n",
    "        X = self.input_dropout(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = self.layers[-1].preprocess_(X) # NOTE: this is identity (exists to replicate tensorflow code)\n",
    "        X = self.output_dense(X).view(shape[:3] + (-1,))\n",
    "\n",
    "        if not sampling and self.hparams.distr == \"cat\": # Unpack the channels\n",
    "            X = X.view(X.shape[0], X.shape[1], X.shape[2] // self.hparams.channels, self.hparams.channels, X.shape[3])\n",
    "            X = X.permute([0, 3, 1, 2, 4])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def loss(self, preds, targets): # Assumes targets have been rescaled to [-1., 1.], categorical dist.\n",
    "        targets = (targets * (NUM_PIXELS - 1)).long()\n",
    "        ce = F.cross_entropy(preds.permute(0, 4, 1, 2, 3), targets, reduction='none')\n",
    "        return ce\n",
    "\n",
    "    def accuracy(self, preds, targets): #for Categorical Distribution\n",
    "        targets = (targets * (NUM_PIXELS - 1)).long()\n",
    "        argmax_preds = torch.argmax(preds, dim=-1)\n",
    "        acc = torch.eq(argmax_preds, targets).float().sum() / np.prod(argmax_preds.shape)\n",
    "        return acc\n",
    "\n",
    "    def sample_from_cat(self, logits, argmax=False):\n",
    "        if argmax:\n",
    "            sel = torch.argmax(logits, -1, keepdim=False).float() / 255.\n",
    "        else:\n",
    "            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) * (1. - 2 * 1e-5) + 1e-5))\n",
    "            sel = torch.argmax(logits + gumbel_noise, -1, keepdim=False).float() / 255.\n",
    "        return sel\n",
    "\n",
    "    def sample(self, n, device, argmax=False):\n",
    "        total_len = (self.hparams.image_size ** 2)\n",
    "        if self.hparams.distr == \"cat\":\n",
    "            total_len *= self.hparams.channels\n",
    "        samples = torch.zeros((n, 3)).to(device)\n",
    "        for curr_infer_length in tqdm(range(total_len)):\n",
    "            outputs = self.forward(samples, sampling=True)\n",
    "            outputs = outputs.view(n, -1, outputs.shape[-1])[:,curr_infer_length:curr_infer_length+1,:]\n",
    "            if self.hparams.distr == \"dmol\":\n",
    "                x = self.sample_from_dmol(outputs).squeeze()\n",
    "            elif self.hparams.distr == \"cat\":\n",
    "                x = self.sample_from_cat(outputs, argmax=argmax)\n",
    "            if curr_infer_length == 0:\n",
    "                samples = x\n",
    "            else:\n",
    "                samples = torch.cat([samples, x], 1)\n",
    "        samples = samples.view(n, self.hparams.image_size, self.hparams.image_size, self.hparams.channels)\n",
    "        samples = samples.permute(0, 3, 1, 2)\n",
    "        return samples\n",
    "\n",
    "    def sample_from_preds(self, preds, argmax=False):\n",
    "        if self.hparams.distr == \"dmol\":\n",
    "            samples = self.sample_from_dmol(preds)\n",
    "            samples = samples.permute(0, 3, 1, 2)\n",
    "        elif self.hparams.distr == \"cat\":\n",
    "            samples = self.sample_from_cat(preds, argmax=argmax)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Implements a single layer of an unconditional ImageTransformer\"\"\"\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.attn = Attn(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.dropout = nn.Dropout(p=hparams.dropout)\n",
    "        self.layernorm_attn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
    "        self.layernorm_ffn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(self.hparams.hidden_size, self.hparams.filter_size, bias=True),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(self.hparams.filter_size, self.hparams.hidden_size, bias=True))\n",
    "\n",
    "    def preprocess_(self, X):\n",
    "        return X\n",
    "\n",
    "    # Takes care of the \"postprocessing\" from tensorflow code with the layernorm and dropout\n",
    "    def forward(self, X):\n",
    "        X = self.preprocess_(X)\n",
    "        y = self.attn(X)\n",
    "        X = self.layernorm_attn(self.dropout(y) + X)\n",
    "        y = self.ffn(self.preprocess_(X))\n",
    "        X = self.layernorm_ffn(self.dropout(y) + X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention (Attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.kd = self.hparams.total_key_depth or self.hparams.hidden_size\n",
    "        self.vd = self.hparams.total_value_depth or self.hparams.hidden_size\n",
    "        self.q_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
    "        self.k_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
    "        self.v_dense = nn.Linear(self.hparams.hidden_size, self.vd, bias=False)\n",
    "        self.output_dense = nn.Linear(self.vd, self.hparams.hidden_size, bias=False)\n",
    "        assert self.kd % self.hparams.num_heads == 0\n",
    "        assert self.vd % self.hparams.num_heads == 0\n",
    "\n",
    "    def dot_product_attention(self, q, k, v, bias=None):\n",
    "        logits = torch.einsum(\"...kd,...qd->...qk\", k, q)\n",
    "        if bias is not None:\n",
    "            logits += bias\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        return weights @ v\n",
    "\n",
    "    def forward(self, X):\n",
    "        q = self.q_dense(X)\n",
    "        k = self.k_dense(X)\n",
    "        v = self.v_dense(X)\n",
    "        # Split to shape [batch_size, num_heads, len, depth / num_heads]\n",
    "        q = q.view(q.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        k = k.view(k.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        v = v.view(v.shape[:-1] + (self.hparams.num_heads, self.vd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        q *= (self.kd // self.hparams.num_heads) ** (-0.5)\n",
    "\n",
    "        if self.hparams.attn_type == \"global\":\n",
    "            bias = -1e9 * torch.triu(torch.ones(X.shape[1], X.shape[1]), 1).to(X.device)\n",
    "            result = self.dot_product_attention(q, k, v, bias=bias)\n",
    "        elif self.hparams.attn_type == \"local_1d\":\n",
    "            len = X.shape[1]\n",
    "            blen = self.hparams.block_length\n",
    "            pad = (0, 0, 0, (-len) % self.hparams.block_length) # Append to multiple of block length\n",
    "            q = F.pad(q, pad)\n",
    "            k = F.pad(k, pad)\n",
    "            v = F.pad(v, pad)\n",
    "\n",
    "            bias = -1e9 * torch.triu(torch.ones(blen, blen), 1).to(X.device)\n",
    "            first_output = self.dot_product_attention(\n",
    "                q[:,:,:blen,:], k[:,:,:blen,:], v[:,:,:blen,:], bias=bias)\n",
    "\n",
    "            if q.shape[2] > blen:\n",
    "                q = q.view(q.shape[0], q.shape[1], -1, blen, q.shape[3])\n",
    "                k = k.view(k.shape[0], k.shape[1], -1, blen, k.shape[3])\n",
    "                v = v.view(v.shape[0], v.shape[1], -1, blen, v.shape[3])\n",
    "                local_k = torch.cat([k[:,:,:-1], k[:,:,1:]], 3) # [batch, nheads, (nblocks - 1), blen * 2, depth]\n",
    "                local_v = torch.cat([v[:,:,:-1], v[:,:,1:]], 3)\n",
    "                tail_q = q[:,:,1:]\n",
    "                bias = -1e9 * torch.triu(torch.ones(blen, 2 * blen), blen + 1).to(X.device)\n",
    "                tail_output = self.dot_product_attention(tail_q, local_k, local_v, bias=bias)\n",
    "                tail_output = tail_output.view(tail_output.shape[0], tail_output.shape[1], -1, tail_output.shape[4])\n",
    "                result = torch.cat([first_output, tail_output], 2)\n",
    "                result = result[:,:,:X.shape[1],:]\n",
    "            else:\n",
    "                result = first_output[:,:,:X.shape[1],:]\n",
    "\n",
    "        result = result.permute([0, 2, 1, 3]).contiguous()\n",
    "        result = result.view(result.shape[0:2] + (-1,))\n",
    "        result = self.output_dense(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.model.image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10('datasets/transformer', transform=transform, download=True)\n",
    "loader = DataLoader(dataset, batch_size=config.train.batch_size, shuffle=True, num_workers=4)\n",
    "input_dim = config.model.image_size ** 2 * config.model.channels\n",
    "model = ImageTransformer(config.model).to(config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1., betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: get_lr(step, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = config.model.initializer_gain\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if \"layernorm\" in name:\n",
    "        continue\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p, gain=np.sqrt(gain)) # Need sqrt for inconsistency between pytorch / TF\n",
    "    else:\n",
    "        a =  np.sqrt(3. * gain / p.shape[0])\n",
    "        nn.init.uniform_(p, -a, a)\n",
    "\n",
    "def revert_samples(input):\n",
    "    if config.model.distr == \"cat\":\n",
    "        return input\n",
    "    elif config.model.distr == \"dmol\":\n",
    "        return input * 0.5 + 0.5\n",
    "\n",
    "step = 0\n",
    "losses_per_dim = torch.zeros(config.model.channels, config.model.image_size, config.model.image_size).to(config.device)\n",
    "\n",
    "for _ in range(config.train.epochs):\n",
    "        for _, (imgs, l) in enumerate(loader):\n",
    "            imgs = imgs.to(config.device)\n",
    "            model.train()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = model.loss(preds, imgs)\n",
    "            \n",
    "            decay = 0. if step == 0 else 0.99\n",
    "            if config.model.distr == \"dmol\":\n",
    "                losses_per_dim[0,:,:] = losses_per_dim[0,:,:] * decay + (1 - decay) * loss.detach().mean(0) / np.log(2)\n",
    "            else:\n",
    "                losses_per_dim = losses_per_dim * decay + (1 - decay) * loss.detach().mean(0) / np.log(2)\n",
    "            loss = loss.view(loss.shape[0], -1).sum(1)\n",
    "            loss = loss.mean(0)\n",
    "\n",
    "            # Show computational graph\n",
    "            # dot = make_dot(loss, dict(model.named_parameters()))\n",
    "            # dot.render('test.gv', view=True)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = (total_norm ** (1. / 2))\n",
    "\n",
    "            if config.train.clip_grad_norm > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), config.train.clip_grad_norm)\n",
    "\n",
    "            total_norm_post = 0\n",
    "            for p in model.parameters():\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm_post += param_norm.item() ** 2\n",
    "            total_norm_post = (total_norm_post ** (1. / 2))\n",
    "\n",
    "            optimizer.step()\n",
    "            bits_per_dim = loss / (np.log(2.) * input_dim)\n",
    "            acc = model.accuracy(preds, imgs)\n",
    "\n",
    "            if step % config.train.log_iter == 0:\n",
    "                logging.info('step: {}; loss: {:.3f}; bits_per_dim: {:.3f}, acc: {:.3f}, grad norm pre: {:.3f}, post: {:.3f}'\n",
    "                             .format(step, loss.item(), bits_per_dim.item(), acc.item(), total_norm, total_norm_post))\n",
    "                tb_logger.add_scalar('loss', loss.item(), global_step=step)\n",
    "                tb_logger.add_scalar('bits_per_dim', bits_per_dim.item(), global_step=step)\n",
    "                tb_logger.add_scalar('acc', acc.item(), global_step=step)\n",
    "                tb_logger.add_scalar('grad_norm', total_norm, global_step=step)\n",
    "\n",
    "            if step % config.train.sample_iter == 0:\n",
    "                logging.info(\"Sampling from model: {}\".format(args.doc))\n",
    "                if config.model.distr == \"cat\":\n",
    "                    channels = ['r','g','b']\n",
    "                    color_codes = ['Reds', \"Greens\", 'Blues']\n",
    "                    for idx, c in enumerate(channels):\n",
    "                        ax = sns.heatmap(losses_per_dim[idx,:,:].cpu().numpy(), linewidth=0.5, cmap=color_codes[idx])\n",
    "                        tb_logger.add_figure(\"losses_per_dim/{}\".format(c), ax.get_figure(), close=True, global_step=step)\n",
    "                else:\n",
    "                    ax = sns.heatmap(losses_per_dim[0,:,:].cpu().numpy(), linewidth=0.5, cmap='Blues')\n",
    "                    tb_logger.add_figure(\"losses_per_dim\", ax.get_figure(), close=True, global_step=step)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    imgs = revert_samples(imgs)\n",
    "                    imgs_grid = torchvision.utils.make_grid(imgs[:8, ...], 3)\n",
    "                    tb_logger.add_image('imgs', imgs_grid, global_step=step)\n",
    "\n",
    "                    # Evaluate model predictions for the input\n",
    "                    pred_samples = revert_samples(model.sample_from_preds(preds))\n",
    "                    pred_samples_grid = torchvision.utils.make_grid(pred_samples[:8, ...], 3)\n",
    "                    tb_logger.add_image('pred_samples/random', pred_samples_grid, global_step=step)\n",
    "                    pred_samples = revert_samples(model.sample_from_preds(preds, argmax=True))\n",
    "                    pred_samples_grid = torchvision.utils.make_grid(pred_samples[:8, ...], 3)\n",
    "                    tb_logger.add_image('pred_samples/argmax', pred_samples_grid, global_step=step)\n",
    "\n",
    "                    if args.sample:\n",
    "                        samples = revert_samples(model.sample(config.train.sample_size, config.device))\n",
    "                        samples_grid = torchvision.utils.make_grid(samples[:8, ...], 3)\n",
    "                        tb_logger.add_image('samples', samples_grid, global_step=step)\n",
    "\n",
    "                    # Argmax samples are not useful for unconditional generation\n",
    "                    # if config.model.distr == \"cat\":\n",
    "                    #     argmax_samples = model.sample(1, config.device, argmax=True)\n",
    "                    #     samples_grid = torchvision.utils.make_grid(argmax_samples[:8, ...], 3)\n",
    "                    #     tb_logger.add_image('argmax_samples', samples_grid, global_step=step)\n",
    "                torch.save(model.state_dict(), os.path.join('transformer_logs', args.doc, \"model.pth\"))\n",
    "            step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MMCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bdb4c9c3f61b306428a8735aa29fdb3d47c708c66b6a0572dccc6544bde0e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
