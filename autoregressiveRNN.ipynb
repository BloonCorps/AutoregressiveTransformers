{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive RNN on Alanine Dipeptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load required modules\n",
    "## load MMCDataset class\n",
    "## MMCD_path = \"/path/to/MMCD\"\n",
    "MMCD_path = \"./\"\n",
    "sys.path.append(MMCD_path)\n",
    "from mmcd import MMCDataset\n",
    "\n",
    "name = 'dialanine'\n",
    "data_path = os.path.join(MMCD_path, \"data\")\n",
    "\n",
    "dataset_train = MMCDataset(root = data_path,\n",
    "                           molecule_name = name,\n",
    "                           train = True,\n",
    "                           coordinate_type = 'internal',\n",
    "                           lazy_load = False)\n",
    "train_loader = DataLoader(dataset_train,\n",
    "                          num_workers = 4,\n",
    "                          batch_size = 256, #256 is the default batch size\n",
    "                          shuffle = True)\n",
    "dataset_test = MMCDataset(root = data_path,\n",
    "                          molecule_name = name,\n",
    "                          train = False,\n",
    "                          coordinate_type = 'internal',\n",
    "                          lazy_load = False)\n",
    "test_loader = DataLoader(dataset_test,\n",
    "                         num_workers = 4,\n",
    "                         batch_size = 256) #256 is the default batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    \"\"\"\n",
    "    Input: data is a single dictionary item. In this case, for dialanine, it is a dictionary item \n",
    "    with six keys, each referencing a torch.tensor of size [256, 256, 256, (256, 19),(256, 19), (256, 19)]\n",
    "    respectively. The batch size is 256, so the batch data is size [256, 256, 256, (256, 19), (256, 19), \n",
    "    (256, 19)]. Each dialiene molecule is  size [1, 1, 1, 19, 19, 19].\n",
    "    \n",
    "    Output: the dictionary item flattened into a 1-D tensor. It is a 1-D tensor of size 60\n",
    "    for dialanine. We want a length 256 data object where each item is a 60-length 1-d tensor\n",
    "    The resulting data object has shape: torch.Size of [256, 60].\n",
    "    \"\"\"\n",
    "    result = torch.cat(\n",
    "        [data['reference_particle_2_bond'][:, None],\n",
    "         data['reference_particle_3_bond'][:, None],\n",
    "         data['reference_particle_3_angle'][:, None],\n",
    "         data['bond'], data['angle'], data['dihedral']],\n",
    "        dim = -1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def rebuild(flat, data_length = 19):\n",
    "    \"\"\"\n",
    "    Input: flattened tensor of torch.Size [256, 60], rebuild and return\n",
    "    the original molecule, a dictionary item. The inverse of the flatten_data function.\n",
    "    \"\"\"\n",
    "    data_length = 19 #this is particular to dialene\n",
    "    result = {}\n",
    "\n",
    "    result['reference_particle_1_xyz'] = torch.zeros((flat.shape[0], 3))\n",
    "    result['reference_particle_2_bond'] = flat[:, 0]\n",
    "    result['reference_particle_3_bond'] = flat[:, 1]\n",
    "    result['reference_particle_3_angle'] = flat[:, 2]\n",
    "\n",
    "    start = 3\n",
    "    end = start + data_length\n",
    "    result['bond'] = flat[:, start:end]\n",
    "\n",
    "    start = end\n",
    "    end = start + data_length\n",
    "    result['angle'] = flat[:, start:end]\n",
    "\n",
    "    start = end\n",
    "    end = start + data_length\n",
    "    result['dihedral'] = flat[:, start:end]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalCoordinateJordanRNNv1(nn.Module):\n",
    "    def __init__(self, batch_size = 256):\n",
    "        super(InternalCoordinateJordanRNNv1, self).__init__()\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        self.hidden_size = 10\n",
    "        self.x_size = 1 #Generating each entry in the Z-matrix at a time. \n",
    "\n",
    "        #Used to generate y_1, which is not conditioned on any input. \n",
    "        self.h_0 = torch.zeros(256, 10) #Torch.zeros(self.batch_size, self.hidden_size)\n",
    "        \n",
    "        #Modules for everything that is not the first input.\n",
    "        self.y_mu_linears = nn.ModuleList([nn.Linear(10, 1) for index in range(60)]) #Means\n",
    "        self.y_sig_linears = nn.ModuleList([nn.Linear(10, 1) for index in range(60)]) #Concentrations\n",
    "        self.h_linears = nn.ModuleList([nn.Linear(3, 10) for index in range(60)])\n",
    "        \n",
    "        #Indexing Rules\n",
    "        #x = [batchsize, values]. Size = [256, 60]\n",
    "        #y = [batchsize, values, iteration]. Size = [256, 2, 60]\n",
    "        #h = [batchsize, values, iteration]. Size = [256, 10, 60]\n",
    "    \n",
    "    def mu_activation(self, y_mu_1):\n",
    "        return 2*np.pi*torch.tanh(y_mu_1)\n",
    "\n",
    "    def sig_activation(self, y_sig_1):\n",
    "        return 100*torch.sigmoid(torch.relu(y_sig_1)) + 0.1\n",
    "\n",
    "    def forward(self, X): \n",
    "        #Notice here that h_0 and y_0 are explicitly calculated. \n",
    "        h = torch.zeros(X.size()[0], self.hidden_size, 1) #[256, 10, 1]\n",
    "\n",
    "        y_mu_1 = self.mu_activation(\n",
    "            self.y_mu_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_sig_1 = self.sig_activation(\n",
    "            self.y_sig_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "        y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "        y = y_cat \n",
    "\n",
    "        for index in range(1, 60, 1): #Recall generating the 0th item, so start at first item.\n",
    "            x_t = X[:, index-1:index] \n",
    "            y_t_minus_1 = y[:, :, index-1]  \n",
    "            x_and_y = torch.cat([x_t, y_t_minus_1], dim=1) #size = [256, 3]\n",
    "\n",
    "            new_h = torch.relu( self.h_linears[index-1](x_and_y) ) #size = [256, 10]\n",
    "            \n",
    "            y_mu_1 = self.mu_activation(\n",
    "                self.y_mu_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_sig_1 = self.sig_activation(\n",
    "                self.y_sig_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "            y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "\n",
    "            new_h = torch.unsqueeze(new_h, dim = 2)\n",
    "            h = torch.cat([h, new_h], dim = 2)\n",
    "            y = torch.cat([y, y_cat], dim = 2)\n",
    "        return h, y\n",
    "\n",
    "    def loss(self, X, Y, tensor_form = False):\n",
    "        #The loss function is the negative log likelihood itself. \n",
    "        q_theta = dist.VonMises(torch.squeeze(Y[:, 0, :]), torch.squeeze(Y[:, 1, :]))\n",
    "\n",
    "        if tensor_form == True:\n",
    "            return q_theta.log_prob(X)\n",
    "\n",
    "        return - torch.sum(q_theta.log_prob(X))\n",
    "    \n",
    "    def sample(self, num_samples = 256):\n",
    "        h = torch.zeros(num_samples, self.hidden_size, 1) #[256, 10, 1]\n",
    "\n",
    "        y_mu_1 = self.mu_activation(\n",
    "            self.y_mu_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_sig_1 = self.sig_activation(\n",
    "            self.y_sig_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "        dist_temp = dist.VonMises(y_cat[:, 0], y_cat[:, 1])\n",
    "\n",
    "        y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "        y = y_cat \n",
    "        \n",
    "        x = dist_temp.sample()\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "       \n",
    "        #Generate the second up to last sample.\n",
    "        for index in range(1, 60, 1): \n",
    "\n",
    "            x_t = x[:, index-1:index] #size = [256, 1]\n",
    "            y_t_minus_1 = y[:, :, index-1] #size = [256, 2]\n",
    "\n",
    "            x_and_y = torch.cat( [x_t, y_t_minus_1], dim = 1 ) #size = [256, 3]\n",
    "            new_h = torch.relu( self.h_linears[index](x_and_y) ) #size = [256, 10]\n",
    "            \n",
    "            y_mu_1 = self.mu_activation(\n",
    "                self.y_mu_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_sig_1 = self.sig_activation(\n",
    "                self.y_sig_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "                \n",
    "            new_dist_temp = dist.VonMises(y_cat[:, 0], y_cat[:, 1])\n",
    "            new_x = new_dist_temp.sample()\n",
    "\n",
    "            h = torch.cat([h, new_h.unsqueeze(dim=2)], dim = 2)\n",
    "\n",
    "            y = torch.cat([y, y_cat.unsqueeze(dim=2)], dim = 2)\n",
    "\n",
    "            x = torch.cat([x, new_x.unsqueeze(dim=1)], dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternalCoordinateJordanRNNv2(nn.Module):\n",
    "    def __init__(self, batch_size = 256):\n",
    "        super(InternalCoordinateJordanRNNv2, self).__init__()\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        self.hidden_size = 10\n",
    "        \n",
    "        #Modules for everything that is not the first input.\n",
    "        self.y_mu_linears = nn.ModuleList([nn.Linear(10, 1) for index in range(60)]) #Means\n",
    "        self.y_sig_linears = nn.ModuleList([nn.Linear(10, 1) for index in range(60)]) #Concentrations\n",
    "        self.h_linears = nn.ModuleList([nn.Linear(3, 10) for index in range(60)])\n",
    "        \n",
    "        #Indexing Rules\n",
    "        #x = [batchsize, values]. Size = [256, 60]\n",
    "        #y = [batchsize, values, iteration]. Size = [256, 2, 60]\n",
    "        #h = [batchsize, values, iteration]. Size = [256, 10, 60]\n",
    "        \n",
    "        self.size = 10\n",
    "        \n",
    "    def mu_activation(self, y_mu_1):\n",
    "        return 2*np.pi*torch.tanh(y_mu_1)\n",
    "\n",
    "    def sig_activation(self, y_sig_1):\n",
    "        return torch.exp(10*torch.tanh(y_sig_1))\n",
    "\n",
    "    def forward(self, X): \n",
    "        #Notice here that h_0 and y_0 are explicitly calculated. \n",
    "        h = torch.zeros(X.size()[0], self.hidden_size, 1).to(device) #[256, 10, 1]\n",
    "\n",
    "        y_mu_1 = self.mu_activation(\n",
    "            self.y_mu_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_sig_1 = self.sig_activation(\n",
    "            self.y_sig_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "        y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "        y = y_cat  \n",
    "\n",
    "        for index in range(1, self.size, 1): #Recall generating the 0th item, so start at first item.\n",
    "            x_t = X[:, index-1:index] #x_0 \n",
    "            y_t_minus_1 = y[:, :, index-1]  #y_0\n",
    "            x_and_y = torch.cat([x_t, y_t_minus_1], dim=1) #size = [256, 3]\n",
    "\n",
    "            new_h = torch.relu( self.h_linears[index-1](x_and_y) ) #size = [256, 10]\n",
    "            \n",
    "            y_mu_1 = self.mu_activation(\n",
    "                self.y_mu_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_sig_1 = self.sig_activation(\n",
    "                self.y_sig_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "            y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "\n",
    "            new_h = torch.unsqueeze(new_h, dim = 2)\n",
    "            h = torch.cat([h, new_h], dim = 2)\n",
    "            y = torch.cat([y, y_cat], dim = 2)\n",
    "        \n",
    "        return h, y\n",
    "    \n",
    "    def sample(self, num_samples = 256):\n",
    "        h = torch.zeros(num_samples, self.hidden_size, 1).to(device) #[256, 10, 1]\n",
    "\n",
    "        y_mu_1 = self.mu_activation(\n",
    "            self.y_mu_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_sig_1 = self.sig_activation(\n",
    "            self.y_sig_linears[0](h[:, :, 0])\n",
    "            )\n",
    "\n",
    "        y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "        dist_temp = dist.Normal(y_cat[:, 0], y_cat[:, 1])\n",
    "\n",
    "        y_cat = torch.unsqueeze(y_cat, dim = 2)\n",
    "        y = y_cat \n",
    "        \n",
    "        x = dist_temp.sample()\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "\n",
    "        #Generate the second up to last sample.\n",
    "        for index in range(1, self.size, 1): \n",
    "            x_t = x[:, index-1:index] #x_0 #size = [256, 1]\n",
    "            y_t_minus_1 = y[:, :, index-1] #x_0 #size = [256, 2]\n",
    "\n",
    "            x_and_y = torch.cat( [x_t, y_t_minus_1], dim = 1 ) #size = [256, 3]\n",
    "            new_h = torch.relu( self.h_linears[index-1](x_and_y) ) #size = [256, 10]\n",
    "            \n",
    "            y_mu_1 = self.mu_activation(\n",
    "                self.y_mu_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_sig_1 = self.sig_activation(\n",
    "                self.y_sig_linears[index](new_h)\n",
    "            )\n",
    "\n",
    "            y_cat = torch.cat([y_mu_1, y_sig_1], dim=1)\n",
    "            \n",
    "            if index < 41:\n",
    "                new_dist_temp = dist.Normal(y_cat[:, 0], y_cat[:, 1])\n",
    "            \n",
    "            if index > 41:\n",
    "                new_dist_temp = dist.VonMises(y_cat[:, 0], y_cat[:, 1])\n",
    "\n",
    "            new_x = new_dist_temp.sample()\n",
    "\n",
    "            h = torch.cat([h, new_h.unsqueeze(dim=2)], dim = 2)\n",
    "            \n",
    "            y = torch.cat([y, y_cat.unsqueeze(dim=2)], dim = 2)\n",
    "\n",
    "            x = torch.cat([x, new_x.unsqueeze(dim=1)], dim = 1)\n",
    "        \n",
    "        #means = y[:, 0, :]\n",
    "        #stds = y[:, 1, :]\n",
    "        return x#, means, stds\n",
    "\n",
    "    def loss(self, X, Y, tensor_form = False):\n",
    "        #The loss function is the negative log likelihood itself. \n",
    "        \n",
    "        q_theta = dist.Normal(loc = Y[:, 0, :41], scale = Y[:, 1, :41])\n",
    "        q_theta_dihedrals = dist.VonMises(loc = Y[:, 0, 41:], concentration = Y[:, 1, 41:])\n",
    "\n",
    "        if tensor_form == True:\n",
    "            return q_theta.log_prob(X[:, :self.size]), q_theta_dihedrals.log_prob(X[:, 41:])\n",
    "\n",
    "        return - torch.sum(q_theta.log_prob(X[:, 0:self.size])) #- torch.sum(q_theta_dihedrals.log_prob(X[:, 41:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = InternalCoordinateJordanRNNv2().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)  #lr is step size, arbitrarily picked.\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        #Flatten the data into a tensor of size (256, 60) tensor before putting it into the GPU\n",
    "        data = flatten_data(data)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h, y = model(data)\n",
    "        loss = model.loss(X = data, Y = y)\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:#args.log_interval == 0: #by default, args.log_interval = 10\n",
    "            print('Train Epoch:', epoch, '[{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "\n",
    "                #len(train_loader) should be 625\n",
    "                batch_idx, #index of the batch we are on \n",
    "                len(train_loader), #how many batches are in the data loader\n",
    "                100. * batch_idx / len(train_loader), #progress percentage\n",
    "                loss.item() / len(train_loader))  #hardcoded batch size\n",
    "                )\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    train(epoch)\n",
    "\n",
    "    '''\n",
    "    sample =  model.sample(1000)\n",
    "    sample = rebuild(sample)\n",
    "    potential_energy_q_theta = dataset_train.compute_potential_energy_for_ic(sample)\n",
    "    print(np.sum(potential_energy_q_theta)/1000)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample From Model to View Distribtuions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(10000)\n",
    "np_samples = np.array(samples.detach().cpu())\n",
    "print(np.shape(np_samples))\n",
    "\n",
    "sample_loader = DataLoader(dataset_train,\n",
    "                          num_workers = 4,\n",
    "                          batch_size = 10000, #256 is the default batch size\n",
    "                          shuffle = True)\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(sample_loader):\n",
    "    data = flatten_data(data)\n",
    "    break\n",
    "\n",
    "np_data = np.array(data)\n",
    "\n",
    "for index in range(0, 10, 1):\n",
    "    plt.hist(np_data[:, index], label = \"True\", bins = 200, alpha = 0.5)\n",
    "    plt.hist(np_samples[:, index], label = \"Learned\", bins = 200, alpha = 0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MMCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bdb4c9c3f61b306428a8735aa29fdb3d47c708c66b6a0572dccc6544bde0e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
