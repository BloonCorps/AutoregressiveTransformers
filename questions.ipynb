{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorboardX\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "#from image_transformer import ImageTransformer\n",
    "import matplotlib\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Slice Two Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = torch.zeros(19, 256, 628)\n",
    "target = torch.zeros(19, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = x_pred.view(-1, 628)\n",
    "target = target.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4864, 628])\n",
      "torch.Size([4864])\n"
     ]
    }
   ],
   "source": [
    "print(x_pred.size())\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4864"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19*256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.zeros(256, 60)\n",
    "print(X[: , 0:2].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Log Probabilites Calculated Elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = torch.zeros(4, 1)\n",
    "y_2 = torch.ones(4, 1)\n",
    "y_com = torch.cat([y_1, y_2], dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_com = torch.unsqueeze(y_com, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_com.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(y_com[:, 0:1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Tensors Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros([256, 10, 2])\n",
    "#y = torch.zeros([256, 2, 2])\n",
    "\n",
    "h_new = torch.zeros([256, 10])\n",
    "#y_new = torch.zeros([256, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Splicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(256, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 59:61].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Stacking Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10]) torch.Size([16, 10])\n",
      "torch.Size([16, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "h_0  = torch.zeros(16, 10)\n",
    "h_1 = torch.zeros(16, 10)\n",
    "\n",
    "print(h_0.size(), h_1.size())\n",
    "h = torch.stack([h_0, h_1], dim = 2)\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[:, :, 1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180],\n",
       "        [ 0.4982, -0.1825, -0.2066,  0.3117, -0.0842,  0.2350, -0.0156,  0.4320,\n",
       "         -0.3399, -0.4180]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(3, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        result = self.linear(torch.cat([X, torch.zeros(64, 1)], 1))\n",
    "        return result\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = MyModule().to(device)\n",
    "model(torch.zeros(64, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModulesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=10, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.linears[0])\n",
    "        #ModuleList can act as an iterable, or be indexed using ints\n",
    "        #For index, layer in enumerate(self.linears):\n",
    "            #print(\"i: \", i), print(\"l: \", l)\n",
    "            #print(\"Hello is anyone there\")\n",
    "            #x = self.linears[index // 2](x) + layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = MyModule().to(device)\n",
    "model(torch.zeros(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#args, config = parse_args_and_config()\n",
    "#if config.model.distr == \"dmol\":\n",
    "model_image_size = 8\n",
    "model_channels = 3\n",
    "model_batch_size = 16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(model_image_size), #model image size, so image is 8 by 8\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10('datasets/transformer', transform=transform , download=True)\n",
    "loader = DataLoader(dataset, batch_size = model_batch_size, shuffle=True, num_workers=4)\n",
    "input_dim = model_image_size ** 2 * model_channels\n",
    "#model = ImageTransformer(config.model).to(config.device)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1., betas=(0.9, 0.98), eps=1e-9)\n",
    "#scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: get_lr(step, config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 8, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "for what, (imgs, l) in enumerate(loader):\n",
    "    imgs = imgs.to(device)\n",
    "    print(type(what))\n",
    "    print(type(imgs))\n",
    "    print(type(l))\n",
    "    break\n",
    "\n",
    "imgs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size is 16, and we have 3 channels (RGB). The images are 32 by 32. But they are shruken down such that they are now 8 by 8. \n",
    "\n",
    "Note that transforms.ToTensor() does the following:\n",
    "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f9ca57c40>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = imgs[0, :, :, :]\n",
    "print(img.size())\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3373, 0.3961, 0.3882, 0.4118, 0.4353, 0.4157, 0.4627, 0.3843],\n",
      "         [0.3765, 0.4902, 0.4588, 0.4510, 0.4627, 0.4784, 0.5333, 0.4078],\n",
      "         [0.4039, 0.4863, 0.4784, 0.4235, 0.4314, 0.5255, 0.5451, 0.4980],\n",
      "         [0.3843, 0.4941, 0.5137, 0.4392, 0.4784, 0.5843, 0.5333, 0.5059],\n",
      "         [0.4314, 0.5176, 0.5137, 0.5608, 0.5529, 0.5804, 0.4627, 0.2941],\n",
      "         [0.2941, 0.4157, 0.5333, 0.6863, 0.6431, 0.6118, 0.4039, 0.1843],\n",
      "         [0.4275, 0.5020, 0.6314, 0.6314, 0.7098, 0.6157, 0.5176, 0.3373],\n",
      "         [0.4941, 0.4196, 0.5176, 0.6431, 0.5333, 0.4353, 0.5529, 0.4745]],\n",
      "\n",
      "        [[0.2863, 0.3294, 0.4627, 0.5294, 0.5922, 0.5176, 0.4431, 0.4784],\n",
      "         [0.3176, 0.3686, 0.4353, 0.5608, 0.5686, 0.4392, 0.4902, 0.5569],\n",
      "         [0.3412, 0.3451, 0.3412, 0.3333, 0.3333, 0.3804, 0.5137, 0.6863],\n",
      "         [0.3490, 0.3373, 0.3529, 0.2980, 0.3294, 0.4039, 0.5059, 0.6667],\n",
      "         [0.4510, 0.3529, 0.3569, 0.3569, 0.3843, 0.4078, 0.3608, 0.2902],\n",
      "         [0.2627, 0.2863, 0.3451, 0.4078, 0.4039, 0.4118, 0.2824, 0.1451],\n",
      "         [0.5255, 0.3961, 0.4510, 0.4471, 0.5255, 0.4431, 0.3569, 0.2314],\n",
      "         [0.6431, 0.3333, 0.3451, 0.4549, 0.4000, 0.3059, 0.3725, 0.3098]],\n",
      "\n",
      "        [[0.1451, 0.1843, 0.4353, 0.5765, 0.6706, 0.5176, 0.3490, 0.4745],\n",
      "         [0.1647, 0.1882, 0.3294, 0.5922, 0.6000, 0.3216, 0.4000, 0.6118],\n",
      "         [0.2000, 0.1529, 0.1490, 0.1922, 0.1922, 0.1843, 0.4549, 0.8392],\n",
      "         [0.2314, 0.1373, 0.1529, 0.1255, 0.1529, 0.1843, 0.4588, 0.8039],\n",
      "         [0.4039, 0.1686, 0.1765, 0.1490, 0.1922, 0.2078, 0.2392, 0.2471],\n",
      "         [0.1647, 0.1294, 0.1490, 0.1608, 0.1765, 0.2000, 0.1373, 0.0549],\n",
      "         [0.5804, 0.2706, 0.2510, 0.2824, 0.3686, 0.2706, 0.1961, 0.1137],\n",
      "         [0.7765, 0.2235, 0.1569, 0.2863, 0.2784, 0.1725, 0.2157, 0.1725]]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [categorical distributions], each of the input pixels' three color channels is encoded using a channel specific set of 256 d-dimensional embedding vectors of the intensity values 0-255. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code up a Pytorch model to generate embeddings..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the official documentation of Pytorch embeddings:\n",
    "\n",
    "num_embeddings (int) – size of the dictionary of embeddings\n",
    "\n",
    "embedding_dim (int) – the size of each embedding vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = model_image_size**2\n",
    "hidden_size = 16\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"\"\"ImageTransformer with DMOL or categorical distribution.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(num_pixels * model_channels, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding \n",
    "\n",
    "n_embeddings, dim = 10, 4\n",
    "\n",
    "emb_1 = Embedding(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 1)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6382],\n",
      "        [ 0.9956],\n",
      "        [ 1.1452],\n",
      "        [-1.6296],\n",
      "        [ 1.3230],\n",
      "        [ 0.5701],\n",
      "        [ 0.9505],\n",
      "        [ 1.3330],\n",
      "        [ 0.6600],\n",
      "        [ 3.3648]], requires_grad=True)\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "torch.Size([2, 1])\n",
      "tensor([[[0.9956]],\n",
      "\n",
      "        [[1.1452]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb_1)\n",
    "print(emb_1.weight)\n",
    "inp =  torch.LongTensor([[1], [2]])\n",
    "print(inp)\n",
    "print(inp.size())\n",
    "embedded = emb_1(inp)\n",
    "embedded.size()\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch[0, 1, 0]#torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5369, -1.4964, -0.6120,  0.6548, -0.2934],\n",
      "        [ 1.3455, -0.7746,  1.6805, -0.2899, -1.2576],\n",
      "        [-0.5600, -0.1753, -0.0259, -0.3302, -0.2651]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9334, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MMCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bdb4c9c3f61b306428a8735aa29fdb3d47c708c66b6a0572dccc6544bde0e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
