{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorboardX\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from image_transformer import ImageTransformer\n",
    "import matplotlib\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchviz import make_dot\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageTransformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PIXELS = 256\n",
    "\n",
    "# Numerically stable implementations\n",
    "def logsoftmax(x):\n",
    "    m = torch.max(x, -1, keepdim=True).values\n",
    "    return x - m - torch.log(torch.exp(x - m).sum(-1, keepdim=True))\n",
    "\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1).values\n",
    "    return m + torch.log(torch.exp(x - m[...,None]).sum(-1))\n",
    "\n",
    "tb_logger = tensorboardX.SummaryWriter(log_dir=os.path.join('transformer_logs', args.doc))\n",
    "\n",
    "class ImageTransformer(nn.Module):\n",
    "    \"\"\"ImageTransformer with DMOL or categorical distribution.\"\"\"\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        #Decoder layers are stacked (nlayers = 12). ModuleList is a way to stack decoder blocks.\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hparams) for _ in range(hparams.nlayers)])\n",
    "\n",
    "        #Dropout = 0.1. Dropout is a way of regularizing NNs by preventing coadaptation.\n",
    "        self.input_dropout = nn.Dropout(p=hparams.dropout)\n",
    "\n",
    "        '''if self.hparams.distr == \"dmol\": # Discretized mixture of logistic, for ordinal valued inputs\n",
    "        assert self.hparams.channels == 3, \"Only supports 3 channels for DML\"\n",
    "        size = (1, self.hparams.channels)\n",
    "        self.embedding_conv = nn.Conv2d(1, self.hparams.hidden_size, kernel_size=size, stride=size)\n",
    "        # 10 = 1 + 2c + c(c-1)/2; if only 1 channel, then 3 total\n",
    "        depth = self.hparams.num_mixtures * 10\n",
    "        self.output_dense = nn.Linear(self.hparams.hidden_size, depth, bias=False)'''\n",
    "\n",
    "        #elif self.hparams.distr == \"cat\": #Categorical\n",
    "        self.embeds = nn.Embedding(NUM_PIXELS * self.hparams.channels, self.hparams.hidden_size)\n",
    "        self.output_dense = nn.Linear(self.hparams.hidden_size, NUM_PIXELS, bias=True)\n",
    "\n",
    "        #I didn't know that Pixels had embeddings as well. \n",
    "        #nn.Linear is just a basic feedforward network. \n",
    "        \n",
    "    def add_timing_signal(self, X, min_timescale=1.0, max_timescale=1.0e4):\n",
    "        '''\n",
    "        Yes, that weird Sin and Cos trick. \n",
    "        '''\n",
    "        num_dims = len(X.shape) - 2 # 2 corresponds to batch and hidden_size dimensions\n",
    "        num_timescales = self.hparams.hidden_size // (num_dims * 2)\n",
    "        log_timescale_increment = np.log(max_timescale / min_timescale) / (num_timescales - 1)\n",
    "        inv_timescales = min_timescale * torch.exp((torch.arange(num_timescales).float() * -log_timescale_increment))\n",
    "        inv_timescales = inv_timescales.to(X.device)\n",
    "        total_signal = torch.zeros_like(X) # Only for debugging purposes\n",
    "        for dim in range(num_dims):\n",
    "            length = X.shape[dim + 1] # add 1 to exclude batch dim\n",
    "            position = torch.arange(length).float().to(X.device)\n",
    "            scaled_time = position.view(-1, 1) * inv_timescales.view(1, -1)\n",
    "            signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)\n",
    "            prepad = dim * 2 * num_timescales\n",
    "            postpad = self.hparams.hidden_size - (dim + 1) * 2 * num_timescales\n",
    "            signal = F.pad(signal, (prepad, postpad))\n",
    "            for _ in range(1 + dim):\n",
    "                signal = signal.unsqueeze(0)\n",
    "            for _ in range(num_dims - 1 - dim):\n",
    "                signal = signal.unsqueeze(-2)\n",
    "            X += signal\n",
    "            total_signal += signal\n",
    "        return X\n",
    "\n",
    "    def shift_and_pad_(self, X):\n",
    "        # Shift inputs over by 1 and pad\n",
    "        shape = X.shape\n",
    "        X = X.view(shape[0], shape[1] * shape[2], shape[3])\n",
    "        X = X[:,:-1,:]\n",
    "        X = F.pad(X, (0, 0, 1, 0)) # Pad second to last dimension\n",
    "        X = X.view(shape)\n",
    "        return X\n",
    "\n",
    "    def forward(self, X, sampling=False):\n",
    "        # Reshape inputs\n",
    "        if sampling:\n",
    "            curr_infer_length = X.shape[1]\n",
    "            row_size = self.hparams.image_size * self.hparams.channels\n",
    "            nrows = curr_infer_length // row_size + 1\n",
    "            X = F.pad(X, (0, nrows * row_size - curr_infer_length))\n",
    "            X = X.view(X.shape[0], -1, row_size)\n",
    "        else:\n",
    "            X = X.permute([0, 2, 3, 1]).contiguous()\n",
    "            X = X.view(X.shape[0], X.shape[1], X.shape[2] * X.shape[3]) # Flatten channels into width\n",
    "\n",
    "        '''# Inputs -> embeddings\n",
    "        if self.hparams.distr == \"dmol\":\n",
    "            # Create a \"channel\" dimension for the 1x3 convolution\n",
    "            # (NOTE: can apply a 1x1 convolution and not reshape, this is for consistency)\n",
    "            X = X.unsqueeze(1)\n",
    "            X = F.relu(self.embedding_conv(X))\n",
    "            X = X.permute([0, 2, 3, 1]) # move channels to the end'''\n",
    "        \n",
    "        #elif self.hparams.distr == \"cat\":\n",
    "        # Convert to indexes, and use separate embeddings for different channels\n",
    "        X = (X * (NUM_PIXELS - 1)).long()\n",
    "        channel_addition = (torch.tensor([0, 1, 2]) * NUM_PIXELS).to(X.device).repeat(X.shape[2] // 3).view(1, 1, -1)\n",
    "        X += channel_addition\n",
    "        X = self.embeds(X) * (self.hparams.hidden_size ** 0.5)\n",
    "\n",
    "        X = self.shift_and_pad_(X)\n",
    "        X = self.add_timing_signal(X)\n",
    "        shape = X.shape\n",
    "        X = X.view(shape[0], -1, shape[3])\n",
    "\n",
    "        X = self.input_dropout(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = self.layers[-1].preprocess_(X) # NOTE: this is identity (exists to replicate tensorflow code)\n",
    "        X = self.output_dense(X).view(shape[:3] + (-1,))\n",
    "\n",
    "        if not sampling and self.hparams.distr == \"cat\": # Unpack the channels\n",
    "            X = X.view(X.shape[0], X.shape[1], X.shape[2] // self.hparams.channels, self.hparams.channels, X.shape[3])\n",
    "            X = X.permute([0, 3, 1, 2, 4])\n",
    "\n",
    "        return X\n",
    "\n",
    "    #def split_to_dml_params(self, preds, targets=None, sampling=False):\n",
    "    #DELETED since this function was only used for DMOL distribution\n",
    "\n",
    "    # Modified from official PixCNN++ code\n",
    "    #def dml_logp(self, logits, means, log_scales, targets):\n",
    "    #DELETED since this function was only used for DMOL distribution\n",
    "\n",
    "    # Assumes targets have been rescaled to [-1., 1.]\n",
    "    def loss(self, preds, targets):\n",
    "        '''if self.hparams.distr == \"dmol\":\n",
    "            # Assumes 3 channels. Input: [batch_size, height, width, 10 * 10]\n",
    "            logits, locs, log_scales = self.split_to_dml_params(preds, targets)\n",
    "            targets = targets.permute([0, 2, 3, 1])\n",
    "            log_probs = self.dml_logp(logits, locs, log_scales, targets)\n",
    "            return -log_probs'''\n",
    "        #elif self.hparams.distr == \"cat\":\n",
    "        targets = (targets * (NUM_PIXELS - 1)).long()\n",
    "        ce = F.cross_entropy(preds.permute(0, 4, 1, 2, 3), targets, reduction='none')\n",
    "        return ce\n",
    "\n",
    "    def accuracy(self, preds, targets):\n",
    "        #for Categorical Distribution\n",
    "        targets = (targets * (NUM_PIXELS - 1)).long()\n",
    "        argmax_preds = torch.argmax(preds, dim=-1)\n",
    "        acc = torch.eq(argmax_preds, targets).float().sum() / np.prod(argmax_preds.shape)\n",
    "        return acc\n",
    "\n",
    "    def sample_from_dmol(self, outputs):\n",
    "        logits, locs, log_scales, coeffs = self.split_to_dml_params(outputs, sampling=True)\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) * (1. - 2 * 1e-5) + 1e-5))\n",
    "        sel = torch.argmax(logits + gumbel_noise, -1, keepdim=True)\n",
    "        one_hot = torch.zeros_like(logits).scatter_(-1, sel, 1).unsqueeze(-2)\n",
    "        locs = (locs * one_hot).sum(-1)\n",
    "        log_scales = (log_scales * one_hot).sum(-1)\n",
    "        coeffs = (coeffs * one_hot).sum(-1)\n",
    "        unif = torch.rand_like(log_scales) * (1. - 2 * 1e-5) + 1e-5\n",
    "        logistic_noise = torch.log(unif) - torch.log1p(-unif)\n",
    "        x = locs + torch.exp(log_scales) * logistic_noise\n",
    "        # NOTE: sampling analogously to pixcnn++, which clamps first, unlike image transformer\n",
    "        x0 = torch.clamp(x[..., 0], -1., 1.)\n",
    "        x1 = torch.clamp(x[..., 1] + coeffs[..., 0] * x0, -1., 1.)\n",
    "        x2 = torch.clamp(x[..., 2] + coeffs[..., 1] * x0 + coeffs[..., 2] * x1, -1., 1.)\n",
    "        x = torch.stack([x0, x1, x2], -1)\n",
    "        return x\n",
    "\n",
    "    def sample_from_cat(self, logits, argmax=False):\n",
    "        if argmax:\n",
    "            sel = torch.argmax(logits, -1, keepdim=False).float() / 255.\n",
    "        else:\n",
    "            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) * (1. - 2 * 1e-5) + 1e-5))\n",
    "            sel = torch.argmax(logits + gumbel_noise, -1, keepdim=False).float() / 255.\n",
    "        return sel\n",
    "\n",
    "    def sample(self, n, device, argmax=False):\n",
    "        total_len = (self.hparams.image_size ** 2)\n",
    "        if self.hparams.distr == \"cat\":\n",
    "            total_len *= self.hparams.channels\n",
    "        samples = torch.zeros((n, 3)).to(device)\n",
    "        for curr_infer_length in tqdm(range(total_len)):\n",
    "            outputs = self.forward(samples, sampling=True)\n",
    "            outputs = outputs.view(n, -1, outputs.shape[-1])[:,curr_infer_length:curr_infer_length+1,:]\n",
    "            if self.hparams.distr == \"dmol\":\n",
    "                x = self.sample_from_dmol(outputs).squeeze()\n",
    "            elif self.hparams.distr == \"cat\":\n",
    "                x = self.sample_from_cat(outputs, argmax=argmax)\n",
    "            if curr_infer_length == 0:\n",
    "                samples = x\n",
    "            else:\n",
    "                samples = torch.cat([samples, x], 1)\n",
    "        samples = samples.view(n, self.hparams.image_size, self.hparams.image_size, self.hparams.channels)\n",
    "        samples = samples.permute(0, 3, 1, 2)\n",
    "        return samples\n",
    "\n",
    "    def sample_from_preds(self, preds, argmax=False):\n",
    "        if self.hparams.distr == \"dmol\":\n",
    "            samples = self.sample_from_dmol(preds)\n",
    "            samples = samples.permute(0, 3, 1, 2)\n",
    "        elif self.hparams.distr == \"cat\":\n",
    "            samples = self.sample_from_cat(preds, argmax=argmax)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Implements a single layer of an unconditional ImageTransformer\"\"\"\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.attn = Attn(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.dropout = nn.Dropout(p=hparams.dropout)\n",
    "        self.layernorm_attn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
    "        self.layernorm_ffn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(self.hparams.hidden_size, self.hparams.filter_size, bias=True),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(self.hparams.filter_size, self.hparams.hidden_size, bias=True))\n",
    "\n",
    "    def preprocess_(self, X):\n",
    "        return X\n",
    "\n",
    "    # Takes care of the \"postprocessing\" from tensorflow code with the layernorm and dropout\n",
    "    def forward(self, X):\n",
    "        X = self.preprocess_(X)\n",
    "        y = self.attn(X)\n",
    "        X = self.layernorm_attn(self.dropout(y) + X)\n",
    "        y = self.ffn(self.preprocess_(X))\n",
    "        X = self.layernorm_ffn(self.dropout(y) + X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.kd = self.hparams.total_key_depth or self.hparams.hidden_size\n",
    "        self.vd = self.hparams.total_value_depth or self.hparams.hidden_size\n",
    "        self.q_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
    "        self.k_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
    "        self.v_dense = nn.Linear(self.hparams.hidden_size, self.vd, bias=False)\n",
    "        self.output_dense = nn.Linear(self.vd, self.hparams.hidden_size, bias=False)\n",
    "        assert self.kd % self.hparams.num_heads == 0\n",
    "        assert self.vd % self.hparams.num_heads == 0\n",
    "\n",
    "    def dot_product_attention(self, q, k, v, bias=None):\n",
    "        logits = torch.einsum(\"...kd,...qd->...qk\", k, q)\n",
    "        if bias is not None:\n",
    "            logits += bias\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        return weights @ v\n",
    "\n",
    "    def forward(self, X):\n",
    "        q = self.q_dense(X)\n",
    "        k = self.k_dense(X)\n",
    "        v = self.v_dense(X)\n",
    "        # Split to shape [batch_size, num_heads, len, depth / num_heads]\n",
    "        q = q.view(q.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        k = k.view(k.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        v = v.view(v.shape[:-1] + (self.hparams.num_heads, self.vd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
    "        q *= (self.kd // self.hparams.num_heads) ** (-0.5)\n",
    "\n",
    "        if self.hparams.attn_type == \"global\":\n",
    "            bias = -1e9 * torch.triu(torch.ones(X.shape[1], X.shape[1]), 1).to(X.device)\n",
    "            result = self.dot_product_attention(q, k, v, bias=bias)\n",
    "        elif self.hparams.attn_type == \"local_1d\":\n",
    "            len = X.shape[1]\n",
    "            blen = self.hparams.block_length\n",
    "            pad = (0, 0, 0, (-len) % self.hparams.block_length) # Append to multiple of block length\n",
    "            q = F.pad(q, pad)\n",
    "            k = F.pad(k, pad)\n",
    "            v = F.pad(v, pad)\n",
    "\n",
    "            bias = -1e9 * torch.triu(torch.ones(blen, blen), 1).to(X.device)\n",
    "            first_output = self.dot_product_attention(\n",
    "                q[:,:,:blen,:], k[:,:,:blen,:], v[:,:,:blen,:], bias=bias)\n",
    "\n",
    "            if q.shape[2] > blen:\n",
    "                q = q.view(q.shape[0], q.shape[1], -1, blen, q.shape[3])\n",
    "                k = k.view(k.shape[0], k.shape[1], -1, blen, k.shape[3])\n",
    "                v = v.view(v.shape[0], v.shape[1], -1, blen, v.shape[3])\n",
    "                local_k = torch.cat([k[:,:,:-1], k[:,:,1:]], 3) # [batch, nheads, (nblocks - 1), blen * 2, depth]\n",
    "                local_v = torch.cat([v[:,:,:-1], v[:,:,1:]], 3)\n",
    "                tail_q = q[:,:,1:]\n",
    "                bias = -1e9 * torch.triu(torch.ones(blen, 2 * blen), blen + 1).to(X.device)\n",
    "                tail_output = self.dot_product_attention(tail_q, local_k, local_v, bias=bias)\n",
    "                tail_output = tail_output.view(tail_output.shape[0], tail_output.shape[1], -1, tail_output.shape[4])\n",
    "                result = torch.cat([first_output, tail_output], 2)\n",
    "                result = result[:,:,:X.shape[1],:]\n",
    "            else:\n",
    "                result = first_output[:,:,:X.shape[1],:]\n",
    "\n",
    "        result = result.permute([0, 2, 1, 3]).contiguous()\n",
    "        result = result.view(result.shape[0:2] + (-1,))\n",
    "        result = self.output_dense(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2namespace(config):\n",
    "    namespace = argparse.Namespace()\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            new_value = dict2namespace(value)\n",
    "        else:\n",
    "            new_value = value\n",
    "        setattr(namespace, key, new_value)\n",
    "    return namespace\n",
    "\n",
    "def parse_args_and_config():\n",
    "    \"\"\"\n",
    "    :return args, config: namespace objects that stores information in args and config files.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=globals()['__doc__'])\n",
    "\n",
    "    parser.add_argument('--config', type=str, default='transformer_tiny.yml', help='Path to the config file')\n",
    "    parser.add_argument('--doc', type=str, default='0', help='A string for documentation purpose')\n",
    "    parser.add_argument('--verbose', type=str, default='info', help='Verbose level: info | debug | warning | critical')\n",
    "    parser.add_argument('--sample', action='store_true', help='Sample at train time')\n",
    "\n",
    "    args, unknown = parser.parse_known_args() #special modification for Jupyter notebooks. \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    args.log = os.path.join('transformer_logs', args.doc)\n",
    "    # parse config file\n",
    "    with open(os.path.join('configs', args.config), 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    new_config = dict2namespace({**config, **vars(args)})\n",
    "\n",
    "    if os.path.exists(args.log):\n",
    "        shutil.rmtree(args.log)\n",
    "\n",
    "    os.makedirs(args.log)\n",
    "\n",
    "    with open(os.path.join(args.log, 'config.yml'), 'w') as f:\n",
    "        yaml.dump(new_config, f, default_flow_style=False)\n",
    "\n",
    "    # setup logger\n",
    "    level = getattr(logging, args.verbose.upper(), None)\n",
    "    if not isinstance(level, int):\n",
    "        raise ValueError('level {} not supported'.format(args.verbose))\n",
    "\n",
    "    handler1 = logging.StreamHandler()\n",
    "    handler2 = logging.FileHandler(os.path.join(args.log, 'stdout.txt'))\n",
    "    formatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')\n",
    "    handler1.setFormatter(formatter)\n",
    "    handler2.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # add device information to args\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    logging.info(\"Using device: {}\".format(device))\n",
    "    new_config.device = device\n",
    "\n",
    "    # set random seed\n",
    "    torch.manual_seed(new_config.seed)\n",
    "    torch.cuda.manual_seed_all(new_config.seed)\n",
    "    np.random.seed(new_config.seed)\n",
    "    logging.info(\"Run name: {}\".format(args.doc))\n",
    "\n",
    "    return args, new_config\n",
    "\n",
    "def get_lr(step, config):\n",
    "    warmup_steps = config.optim.warmup\n",
    "    lr_base = config.optim.lr * 0.002 # for Adam correction\n",
    "    ret = 5000. * config.model.hidden_size ** (-0.5) * \\\n",
    "          np.min([(step + 1) * warmup_steps ** (-1.5), (step + 1) ** (-0.5)])\n",
    "    return ret * lr_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,857 - Using device: cuda\n",
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,857 - Using device: cuda\n",
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,857 - Using device: cuda\n",
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,862 - Run name: 0\n",
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,862 - Run name: 0\n",
      "INFO - <ipython-input-27-fea14b691037> - 2022-06-27 23:27:49,862 - Run name: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "args, config = parse_args_and_config()\n",
    "\n",
    "if config.model.distr == \"dmol\":\n",
    "    # Scale size and rescale data to [-1, 1]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.model.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                            std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.model.image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "dataset = datasets.CIFAR10('datasets/transformer', transform=transform, download=True)\n",
    "loader = DataLoader(dataset, batch_size=config.train.batch_size, shuffle=True, num_workers=4)\n",
    "input_dim = config.model.image_size ** 2 * config.model.channels\n",
    "model = ImageTransformer(config.model).to(config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1., betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: get_lr(step, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,070 - step: 0; loss: 1072.975; bits_per_dim: 8.062, acc: 0.004, grad norm pre: 78.396, post: 78.396\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,070 - step: 0; loss: 1072.975; bits_per_dim: 8.062, acc: 0.004, grad norm pre: 78.396, post: 78.396\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,070 - step: 0; loss: 1072.975; bits_per_dim: 8.062, acc: 0.004, grad norm pre: 78.396, post: 78.396\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,074 - Sampling from model: 0\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,074 - Sampling from model: 0\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:52,074 - Sampling from model: 0\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,016 - step: 10; loss: 1072.973; bits_per_dim: 8.062, acc: 0.005, grad norm pre: 88.140, post: 88.140\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,016 - step: 10; loss: 1072.973; bits_per_dim: 8.062, acc: 0.005, grad norm pre: 88.140, post: 88.140\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,016 - step: 10; loss: 1072.973; bits_per_dim: 8.062, acc: 0.005, grad norm pre: 88.140, post: 88.140\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,514 - step: 20; loss: 1070.911; bits_per_dim: 8.047, acc: 0.005, grad norm pre: 101.781, post: 101.781\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,514 - step: 20; loss: 1070.911; bits_per_dim: 8.047, acc: 0.005, grad norm pre: 101.781, post: 101.781\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:53,514 - step: 20; loss: 1070.911; bits_per_dim: 8.047, acc: 0.005, grad norm pre: 101.781, post: 101.781\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,041 - step: 30; loss: 1071.739; bits_per_dim: 8.053, acc: 0.004, grad norm pre: 81.722, post: 81.722\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,041 - step: 30; loss: 1071.739; bits_per_dim: 8.053, acc: 0.004, grad norm pre: 81.722, post: 81.722\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,041 - step: 30; loss: 1071.739; bits_per_dim: 8.053, acc: 0.004, grad norm pre: 81.722, post: 81.722\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,535 - step: 40; loss: 1065.663; bits_per_dim: 8.007, acc: 0.005, grad norm pre: 95.686, post: 95.686\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,535 - step: 40; loss: 1065.663; bits_per_dim: 8.007, acc: 0.005, grad norm pre: 95.686, post: 95.686\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:54,535 - step: 40; loss: 1065.663; bits_per_dim: 8.007, acc: 0.005, grad norm pre: 95.686, post: 95.686\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,002 - step: 50; loss: 1066.820; bits_per_dim: 8.016, acc: 0.007, grad norm pre: 82.695, post: 82.695\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,002 - step: 50; loss: 1066.820; bits_per_dim: 8.016, acc: 0.007, grad norm pre: 82.695, post: 82.695\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,002 - step: 50; loss: 1066.820; bits_per_dim: 8.016, acc: 0.007, grad norm pre: 82.695, post: 82.695\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,484 - step: 60; loss: 1062.832; bits_per_dim: 7.986, acc: 0.008, grad norm pre: 82.214, post: 82.214\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,484 - step: 60; loss: 1062.832; bits_per_dim: 7.986, acc: 0.008, grad norm pre: 82.214, post: 82.214\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:55,484 - step: 60; loss: 1062.832; bits_per_dim: 7.986, acc: 0.008, grad norm pre: 82.214, post: 82.214\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,005 - step: 70; loss: 1063.036; bits_per_dim: 7.988, acc: 0.005, grad norm pre: 89.857, post: 89.857\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,005 - step: 70; loss: 1063.036; bits_per_dim: 7.988, acc: 0.005, grad norm pre: 89.857, post: 89.857\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,005 - step: 70; loss: 1063.036; bits_per_dim: 7.988, acc: 0.005, grad norm pre: 89.857, post: 89.857\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,475 - step: 80; loss: 1061.375; bits_per_dim: 7.975, acc: 0.007, grad norm pre: 88.501, post: 88.501\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,475 - step: 80; loss: 1061.375; bits_per_dim: 7.975, acc: 0.007, grad norm pre: 88.501, post: 88.501\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,475 - step: 80; loss: 1061.375; bits_per_dim: 7.975, acc: 0.007, grad norm pre: 88.501, post: 88.501\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,958 - step: 90; loss: 1065.999; bits_per_dim: 8.010, acc: 0.008, grad norm pre: 75.169, post: 75.169\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,958 - step: 90; loss: 1065.999; bits_per_dim: 8.010, acc: 0.008, grad norm pre: 75.169, post: 75.169\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:56,958 - step: 90; loss: 1065.999; bits_per_dim: 8.010, acc: 0.008, grad norm pre: 75.169, post: 75.169\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,440 - step: 100; loss: 1054.234; bits_per_dim: 7.922, acc: 0.010, grad norm pre: 75.796, post: 75.796\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,440 - step: 100; loss: 1054.234; bits_per_dim: 7.922, acc: 0.010, grad norm pre: 75.796, post: 75.796\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,440 - step: 100; loss: 1054.234; bits_per_dim: 7.922, acc: 0.010, grad norm pre: 75.796, post: 75.796\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,927 - step: 110; loss: 1048.411; bits_per_dim: 7.878, acc: 0.012, grad norm pre: 74.757, post: 74.757\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,927 - step: 110; loss: 1048.411; bits_per_dim: 7.878, acc: 0.012, grad norm pre: 74.757, post: 74.757\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:57,927 - step: 110; loss: 1048.411; bits_per_dim: 7.878, acc: 0.012, grad norm pre: 74.757, post: 74.757\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,449 - step: 120; loss: 1055.817; bits_per_dim: 7.933, acc: 0.007, grad norm pre: 75.410, post: 75.410\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,449 - step: 120; loss: 1055.817; bits_per_dim: 7.933, acc: 0.007, grad norm pre: 75.410, post: 75.410\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,449 - step: 120; loss: 1055.817; bits_per_dim: 7.933, acc: 0.007, grad norm pre: 75.410, post: 75.410\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,933 - step: 130; loss: 1045.901; bits_per_dim: 7.859, acc: 0.010, grad norm pre: 75.633, post: 75.633\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,933 - step: 130; loss: 1045.901; bits_per_dim: 7.859, acc: 0.010, grad norm pre: 75.633, post: 75.633\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:58,933 - step: 130; loss: 1045.901; bits_per_dim: 7.859, acc: 0.010, grad norm pre: 75.633, post: 75.633\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,409 - step: 140; loss: 1064.614; bits_per_dim: 8.000, acc: 0.006, grad norm pre: 63.868, post: 63.868\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,409 - step: 140; loss: 1064.614; bits_per_dim: 8.000, acc: 0.006, grad norm pre: 63.868, post: 63.868\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,409 - step: 140; loss: 1064.614; bits_per_dim: 8.000, acc: 0.006, grad norm pre: 63.868, post: 63.868\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,974 - step: 150; loss: 1048.485; bits_per_dim: 7.878, acc: 0.003, grad norm pre: 78.702, post: 78.702\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,974 - step: 150; loss: 1048.485; bits_per_dim: 7.878, acc: 0.003, grad norm pre: 78.702, post: 78.702\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:27:59,974 - step: 150; loss: 1048.485; bits_per_dim: 7.878, acc: 0.003, grad norm pre: 78.702, post: 78.702\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,472 - step: 160; loss: 1062.052; bits_per_dim: 7.980, acc: 0.004, grad norm pre: 57.790, post: 57.790\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,472 - step: 160; loss: 1062.052; bits_per_dim: 7.980, acc: 0.004, grad norm pre: 57.790, post: 57.790\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,472 - step: 160; loss: 1062.052; bits_per_dim: 7.980, acc: 0.004, grad norm pre: 57.790, post: 57.790\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,991 - step: 170; loss: 1053.605; bits_per_dim: 7.917, acc: 0.010, grad norm pre: 58.409, post: 58.409\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,991 - step: 170; loss: 1053.605; bits_per_dim: 7.917, acc: 0.010, grad norm pre: 58.409, post: 58.409\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:00,991 - step: 170; loss: 1053.605; bits_per_dim: 7.917, acc: 0.010, grad norm pre: 58.409, post: 58.409\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:01,496 - step: 180; loss: 1044.386; bits_per_dim: 7.848, acc: 0.008, grad norm pre: 54.373, post: 54.373\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:01,496 - step: 180; loss: 1044.386; bits_per_dim: 7.848, acc: 0.008, grad norm pre: 54.373, post: 54.373\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:01,496 - step: 180; loss: 1044.386; bits_per_dim: 7.848, acc: 0.008, grad norm pre: 54.373, post: 54.373\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,036 - step: 190; loss: 1037.146; bits_per_dim: 7.793, acc: 0.007, grad norm pre: 68.543, post: 68.543\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,036 - step: 190; loss: 1037.146; bits_per_dim: 7.793, acc: 0.007, grad norm pre: 68.543, post: 68.543\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,036 - step: 190; loss: 1037.146; bits_per_dim: 7.793, acc: 0.007, grad norm pre: 68.543, post: 68.543\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,600 - step: 200; loss: 1047.508; bits_per_dim: 7.871, acc: 0.005, grad norm pre: 54.809, post: 54.809\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,600 - step: 200; loss: 1047.508; bits_per_dim: 7.871, acc: 0.005, grad norm pre: 54.809, post: 54.809\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:02,600 - step: 200; loss: 1047.508; bits_per_dim: 7.871, acc: 0.005, grad norm pre: 54.809, post: 54.809\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,155 - step: 210; loss: 1029.365; bits_per_dim: 7.735, acc: 0.007, grad norm pre: 78.954, post: 78.954\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,155 - step: 210; loss: 1029.365; bits_per_dim: 7.735, acc: 0.007, grad norm pre: 78.954, post: 78.954\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,155 - step: 210; loss: 1029.365; bits_per_dim: 7.735, acc: 0.007, grad norm pre: 78.954, post: 78.954\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,719 - step: 220; loss: 1051.621; bits_per_dim: 7.902, acc: 0.003, grad norm pre: 72.117, post: 72.117\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,719 - step: 220; loss: 1051.621; bits_per_dim: 7.902, acc: 0.003, grad norm pre: 72.117, post: 72.117\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:03,719 - step: 220; loss: 1051.621; bits_per_dim: 7.902, acc: 0.003, grad norm pre: 72.117, post: 72.117\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:04,209 - step: 230; loss: 1024.005; bits_per_dim: 7.694, acc: 0.010, grad norm pre: 62.929, post: 62.929\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:04,209 - step: 230; loss: 1024.005; bits_per_dim: 7.694, acc: 0.010, grad norm pre: 62.929, post: 62.929\n",
      "INFO - <ipython-input-29-44ee206a9f55> - 2022-06-27 23:28:04,209 - step: 230; loss: 1024.005; bits_per_dim: 7.694, acc: 0.010, grad norm pre: 62.929, post: 62.929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-44ee206a9f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtotal_norm_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm_post\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mbits_per_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MMCD/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gain = config.model.initializer_gain\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if \"layernorm\" in name:\n",
    "        continue\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p, gain=np.sqrt(gain)) # Need sqrt for inconsistency between pytorch / TF\n",
    "    else:\n",
    "        a =  np.sqrt(3. * gain / p.shape[0])\n",
    "        nn.init.uniform_(p, -a, a)\n",
    "\n",
    "def revert_samples(input):\n",
    "    if config.model.distr == \"cat\":\n",
    "        return input\n",
    "    elif config.model.distr == \"dmol\":\n",
    "        return input * 0.5 + 0.5\n",
    "\n",
    "step = 0\n",
    "losses_per_dim = torch.zeros(config.model.channels, config.model.image_size, config.model.image_size).to(config.device)\n",
    "\n",
    "for _ in range(config.train.epochs):\n",
    "        for _, (imgs, l) in enumerate(loader):\n",
    "            imgs = imgs.to(config.device)\n",
    "            model.train()\n",
    "\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            loss = model.loss(preds, imgs)\n",
    "            decay = 0. if step == 0 else 0.99\n",
    "            if config.model.distr == \"dmol\":\n",
    "                losses_per_dim[0,:,:] = losses_per_dim[0,:,:] * decay + (1 - decay) * loss.detach().mean(0) / np.log(2)\n",
    "            else:\n",
    "                losses_per_dim = losses_per_dim * decay + (1 - decay) * loss.detach().mean(0) / np.log(2)\n",
    "            loss = loss.view(loss.shape[0], -1).sum(1)\n",
    "            loss = loss.mean(0)\n",
    "\n",
    "            # Show computational graph\n",
    "            # dot = make_dot(loss, dict(model.named_parameters()))\n",
    "            # dot.render('test.gv', view=True)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = (total_norm ** (1. / 2))\n",
    "\n",
    "            if config.train.clip_grad_norm > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), config.train.clip_grad_norm)\n",
    "\n",
    "            total_norm_post = 0\n",
    "            for p in model.parameters():\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm_post += param_norm.item() ** 2\n",
    "            total_norm_post = (total_norm_post ** (1. / 2))\n",
    "\n",
    "            optimizer.step()\n",
    "            bits_per_dim = loss / (np.log(2.) * input_dim)\n",
    "            acc = model.accuracy(preds, imgs)\n",
    "\n",
    "            if step % config.train.log_iter == 0:\n",
    "                logging.info('step: {}; loss: {:.3f}; bits_per_dim: {:.3f}, acc: {:.3f}, grad norm pre: {:.3f}, post: {:.3f}'\n",
    "                             .format(step, loss.item(), bits_per_dim.item(), acc.item(), total_norm, total_norm_post))\n",
    "                tb_logger.add_scalar('loss', loss.item(), global_step=step)\n",
    "                tb_logger.add_scalar('bits_per_dim', bits_per_dim.item(), global_step=step)\n",
    "                tb_logger.add_scalar('acc', acc.item(), global_step=step)\n",
    "                tb_logger.add_scalar('grad_norm', total_norm, global_step=step)\n",
    "\n",
    "            if step % config.train.sample_iter == 0:\n",
    "                logging.info(\"Sampling from model: {}\".format(args.doc))\n",
    "                if config.model.distr == \"cat\":\n",
    "                    channels = ['r','g','b']\n",
    "                    color_codes = ['Reds', \"Greens\", 'Blues']\n",
    "                    for idx, c in enumerate(channels):\n",
    "                        ax = sns.heatmap(losses_per_dim[idx,:,:].cpu().numpy(), linewidth=0.5, cmap=color_codes[idx])\n",
    "                        tb_logger.add_figure(\"losses_per_dim/{}\".format(c), ax.get_figure(), close=True, global_step=step)\n",
    "                else:\n",
    "                    ax = sns.heatmap(losses_per_dim[0,:,:].cpu().numpy(), linewidth=0.5, cmap='Blues')\n",
    "                    tb_logger.add_figure(\"losses_per_dim\", ax.get_figure(), close=True, global_step=step)\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    imgs = revert_samples(imgs)\n",
    "                    imgs_grid = torchvision.utils.make_grid(imgs[:8, ...], 3)\n",
    "                    tb_logger.add_image('imgs', imgs_grid, global_step=step)\n",
    "\n",
    "                    # Evaluate model predictions for the input\n",
    "                    pred_samples = revert_samples(model.sample_from_preds(preds))\n",
    "                    pred_samples_grid = torchvision.utils.make_grid(pred_samples[:8, ...], 3)\n",
    "                    tb_logger.add_image('pred_samples/random', pred_samples_grid, global_step=step)\n",
    "                    pred_samples = revert_samples(model.sample_from_preds(preds, argmax=True))\n",
    "                    pred_samples_grid = torchvision.utils.make_grid(pred_samples[:8, ...], 3)\n",
    "                    tb_logger.add_image('pred_samples/argmax', pred_samples_grid, global_step=step)\n",
    "\n",
    "                    if args.sample:\n",
    "                        samples = revert_samples(model.sample(config.train.sample_size, config.device))\n",
    "                        samples_grid = torchvision.utils.make_grid(samples[:8, ...], 3)\n",
    "                        tb_logger.add_image('samples', samples_grid, global_step=step)\n",
    "\n",
    "                    # Argmax samples are not useful for unconditional generation\n",
    "                    # if config.model.distr == \"cat\":\n",
    "                    #     argmax_samples = model.sample(1, config.device, argmax=True)\n",
    "                    #     samples_grid = torchvision.utils.make_grid(argmax_samples[:8, ...], 3)\n",
    "                    #     tb_logger.add_image('argmax_samples', samples_grid, global_step=step)\n",
    "                torch.save(model.state_dict(), os.path.join('transformer_logs', args.doc, \"model.pth\"))\n",
    "            step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MMCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bdb4c9c3f61b306428a8735aa29fdb3d47c708c66b6a0572dccc6544bde0e67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
