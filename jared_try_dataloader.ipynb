{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec48b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/jzheng/.conda/envs/UROP/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a baseline autoregressive model that uses a causal LM approach to generating\n",
    "series of angles\n",
    "\"\"\"\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.append('/bin/')\n",
    "sys.path.append('bin')\n",
    "sys.path.append('bin/')\n",
    "sys.path.append('/bin')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import multiprocessing\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "from foldingdiff import datasets, modelling, losses, plotting, utils\n",
    "from foldingdiff import custom_metrics as cm\n",
    "\n",
    "from train import ANGLES_DEFINITIONS, build_callbacks, record_args_and_metadata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import torch.optim as optim\n",
    "from torch import nn, einsum\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#my codes\n",
    "import transformer\n",
    "import utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d59fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_test_sets(\n",
    "    angles_definitions: ANGLES_DEFINITIONS = \"canonical-full-angles\",\n",
    "    max_seq_len: int = 512,\n",
    "    min_seq_len: int = 0,\n",
    "    seq_trim_strategy: datasets.TRIM_STRATEGIES = \"leftalign\",\n",
    ") -> Tuple[\n",
    "    datasets.AutoregressiveCausalDataset,\n",
    "    datasets.AutoregressiveCausalDataset,\n",
    "    datasets.AutoregressiveCausalDataset,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Get the train/valid/test splits using the autoregressive wrapper on the datsets\n",
    "    \"\"\"\n",
    "\n",
    "    clean_dset_class = {\n",
    "        \"canonical\": datasets.CathCanonicalAnglesDataset,\n",
    "        \"canonical-full-angles\": datasets.CathCanonicalAnglesOnlyDataset,\n",
    "        \"canonical-minimal-angles\": datasets.CathCanonicalMinimalAnglesDataset,\n",
    "        \"cart-coords\": datasets.CathCanonicalCoordsDataset,\n",
    "    }[angles_definitions]\n",
    "    logging.info(f\"Clean dataset class: {clean_dset_class}\")\n",
    "\n",
    "    splits = [\"train\", \"validation\", \"test\"]\n",
    "    logging.info(f\"Creating data splits: {splits}\")\n",
    "    clean_dsets = [\n",
    "        clean_dset_class(\n",
    "            split=s,\n",
    "            pad=max_seq_len,\n",
    "            min_length=min_seq_len,\n",
    "            trim_strategy=seq_trim_strategy,\n",
    "            zero_center=False if angles_definitions == \"cart-coords\" else True,\n",
    "        )\n",
    "        for s in splits\n",
    "    ]\n",
    "\n",
    "    # Set the training set mean to the validation set mean\n",
    "    if len(clean_dsets) > 1 and clean_dsets[0].means is not None:\n",
    "        logging.info(f\"Updating valid/test mean offset to {clean_dsets[0].means}\")\n",
    "        for i in range(1, len(clean_dsets)):\n",
    "            clean_dsets[i].means = clean_dsets[0].means\n",
    "\n",
    "    causal_dsets = [\n",
    "        datasets.AutoregressiveCausalDataset(\n",
    "            d, dset_key=\"coords\" if angles_definitions == \"cart-coords\" else \"angles\"\n",
    "        )\n",
    "        for d in clean_dsets\n",
    "    ]\n",
    "    for dsname, ds in zip(splits, causal_dsets):\n",
    "        logging.info(f\"{dsname}: {ds}\")\n",
    "    return causal_dsets\n",
    "\n",
    "def return_dataset(\n",
    "    ### Well, really only returns a dataset. \n",
    "    results_dir: str = \"./results\",\n",
    "    angles_definitions: ANGLES_DEFINITIONS = \"canonical-full-angles\",\n",
    "    max_seq_len: int = 128,\n",
    "    min_seq_len: int = 0,\n",
    "    trim_strategy: datasets.TRIM_STRATEGIES = \"randomcrop\",\n",
    "    # Related to model architecture\n",
    "    seq_len_encoding: modelling.TIME_ENCODING = \"gaussian_fourier\",  # Embeds the total sequence length\n",
    "    num_hidden_layers: int = 12,  # Default 12\n",
    "    hidden_size: int = 384,  # Default 768\n",
    "    intermediate_size: int = 768,  # Default 3072\n",
    "    num_heads: int = 12,  # Default 12\n",
    "    position_embedding_type: Literal[\n",
    "        \"absolute\", \"relative_key_query\", \"relative_key\"\n",
    "    ] = \"absolute\",  # Default absolute\n",
    "    dropout_p: float = 0.1,\n",
    "    decoder: modelling.DECODER_HEAD = \"mlp\",\n",
    "    # Related to training strategy\n",
    "    gradient_clip: float = 1.0,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 5e-5,\n",
    "    l2_norm: float = 0.01,\n",
    "    loss: modelling.LOSS_KEYS = \"smooth_l1\",\n",
    "    min_epochs: Optional[int] = None,\n",
    "    max_epochs: int = 10000,  # 10000, set to 100 for debug\n",
    "    early_stop_patience: int = 0,  # Set to 0 to disable early stopping\n",
    "    lr_scheduler: modelling.LR_SCHEDULE = \"LinearWarmup\",  # Try LinearWarmup?\n",
    "    use_swa: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    func_args = locals()\n",
    "\n",
    "    ft_key = \"coords\" if angles_definitions == \"cart-coords\" else \"angles\"\n",
    "    dsets = get_train_valid_test_sets(\n",
    "        angles_definitions=angles_definitions,\n",
    "        max_seq_len=max_seq_len,\n",
    "        min_seq_len=min_seq_len,\n",
    "        seq_trim_strategy=trim_strategy,\n",
    "    )\n",
    "    assert len(dsets) == 3\n",
    "\n",
    "    # Calculate effective batch size\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.4.0/advanced/multi_gpu.html#batch-size\n",
    "    # Under DDP, effective batch size is batch_size * num_gpus * num_nodes\n",
    "    effective_batch_size = batch_size\n",
    "    if torch.cuda.is_available():\n",
    "        effective_batch_size = int(batch_size / torch.cuda.device_count())\n",
    "    pl.utilities.rank_zero_info(\n",
    "        f\"Given batch size: {batch_size} --> effective batch size with {torch.cuda.device_count()} GPUs: {effective_batch_size}\"\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader, valid_dataloader, test_dataloader = [\n",
    "        DataLoader(\n",
    "            dataset=ds,\n",
    "            batch_size=effective_batch_size,\n",
    "            shuffle=i == 0,  # Shuffle only train loader\n",
    "            num_workers=multiprocessing.cpu_count(),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        for i, ds in enumerate(dsets)\n",
    "    ]\n",
    "\n",
    "    logging.info(f\"Using loss function: {loss}\")\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aec5119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Given batch size: 32 --> effective batch size with 1 GPUs: 32\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = return_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bbee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_NUM_BINS = 600\n",
    "GLOBAL_BATCH_SIZE = 157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930dc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = transformer.UnconditionalTransformer(seq_len=768, hidden_size=512, num_bins=GLOBAL_NUM_BINS, \n",
    "                             dropout=0.05, dnlayers=4, batch_size=GLOBAL_BATCH_SIZE, \n",
    "                             ffn_hidden_size=1024, num_heads=1, qk_depth=128, \n",
    "                             v_depth=128, pseudolikelihood=True, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23b841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32])\n",
      "1\n",
      "torch.Size([32])\n",
      "2\n",
      "torch.Size([32])\n",
      "3\n",
      "torch.Size([32])\n",
      "4\n",
      "torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data_dict in enumerate(train_dataloader):\n",
    "    print(batch_idx)\n",
    "    \n",
    "    k = data_dict['lengths']\n",
    "    print(k.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d107851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157, 256, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2589154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1 (0%)]\tLoss: 0.028811\n",
      "Train Epoch: 1 [0/1 (0%)]\tLoss: 0.029445\n",
      "Train Epoch: 2 [0/1 (0%)]\tLoss: 0.027539\n",
      "Train Epoch: 3 [0/1 (0%)]\tLoss: 0.027233\n",
      "Train Epoch: 4 [0/1 (0%)]\tLoss: 0.027227\n",
      "Train Epoch: 5 [0/1 (0%)]\tLoss: 0.026786\n",
      "Train Epoch: 6 [0/1 (0%)]\tLoss: 0.026225\n",
      "Train Epoch: 7 [0/1 (0%)]\tLoss: 0.025823\n",
      "Train Epoch: 8 [0/1 (0%)]\tLoss: 0.025699\n",
      "Train Epoch: 9 [0/1 (0%)]\tLoss: 0.025606\n",
      "Train Epoch: 10 [0/1 (0%)]\tLoss: 0.025386\n",
      "Train Epoch: 11 [0/1 (0%)]\tLoss: 0.025064\n",
      "Train Epoch: 12 [0/1 (0%)]\tLoss: 0.024847\n",
      "Train Epoch: 13 [0/1 (0%)]\tLoss: 0.024702\n",
      "Train Epoch: 14 [0/1 (0%)]\tLoss: 0.024571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     48\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(epoch):\n\u001b[1;32m      8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     11\u001b[0m         data \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangles\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m         data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(data, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1315\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1317\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.conda/envs/UROP/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_over_time = []\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max = 10, eta_min=0, last_epoch= -1, verbose=False)\n",
    "\n",
    "def train_loop(epoch):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data_dict in enumerate(train_dataloader):\n",
    "        \n",
    "        data = data_dict['angles'].to(device)\n",
    "        data = torch.flatten(data, start_dim=1)\n",
    "        \n",
    "        target = utilities.custom_bucketize(data, GLOBAL_NUM_BINS).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        softmax_X_pred, attn_mats = model(decoder_input=data, return_attention=True) \n",
    "        #softmax_X_pred = model(decoder_input=target)#, return_attention=True) \n",
    "        \n",
    "        loss = model.loss(X=softmax_X_pred, Y=target) + 1e-9*torch.sum(torch.abs(attn_mats))\n",
    "            \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        #clip grad norm\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:#args.log_interval == 0: #by default, args.log_interval = 10\n",
    "            print('Train Epoch:', epoch, '[{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx, #index of the batch we are on \n",
    "                len(train_dataloader), #how many batches are in the data loader\n",
    "                100. * batch_idx / len(train_dataloader), #progress percentage\n",
    "                loss.item() / GLOBAL_BATCH_SIZE#,  #hardcoded batch size\n",
    "                ))\n",
    "            loss_over_time.append(loss.item()/len(train_dataloader) )\n",
    "            torch.set_printoptions(threshold=10_000)\n",
    "            \n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(0, 20, 1):\n",
    "    train_loop(epoch)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2590f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-UROP]",
   "language": "python",
   "name": "conda-env-.conda-UROP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
